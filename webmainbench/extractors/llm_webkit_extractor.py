"""
LLM-WebKit extractor implementation with advanced LLM inference.
"""

import json
import re
import time
from typing import Dict, Any, Optional, List
from enum import Enum
from dataclasses import dataclass
import torch

from .base import BaseExtractor, ExtractionResult
from .factory import extractor


@dataclass
class LLMInferenceConfig:
    """Configuration for LLM inference."""
    model_path: str = "/path/to/your/model"
    use_logits_processor: bool = True
    max_tokens: int = 32768         # æœ€å¤§è¾“å…¥tokenæ•°
    temperature: float = 0.0
    top_p: float = 0.95
    max_output_tokens: int = 8192   # æœ€å¤§è¾“å‡ºtokenæ•°
    tensor_parallel_size: int = 1   # å¼ é‡å¹¶è¡Œå¤§å°
    dtype: str = "bfloat16"         # æ•°æ®ç±»å‹
    max_item_count: int = 1000      # æœ€å¤§itemæ•°é‡
    gpu_memory_utilization: float = 0.8  # GPUå†…å­˜åˆ©ç”¨ç‡
    enforce_eager: bool = True      # ä½¿ç”¨eageræ¨¡å¼


class TokenState(Enum):
    """Token states for JSON format enforcement."""
    Left_bracket = 0
    Right_bracket = 1
    Space_quote = 2
    Quote_colon_quote = 3
    Quote_comma = 4
    Main_other = 5
    Number = 6


class TokenStateManager:
    """Manages token states to ensure valid JSON output."""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        token_id_map = {
            TokenState.Left_bracket: ["{"],
            TokenState.Right_bracket: ["}"],
            TokenState.Space_quote: [' "'],
            TokenState.Quote_colon_quote: ['":"'],
            TokenState.Quote_comma: ['",'],
            TokenState.Main_other: ["main", "other"],
            TokenState.Number: ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"],
        }
        self.token_id_map = {k: [self.tokenizer.encode(v)[0] for v in token_id_map[k]] for k in token_id_map}
    
    def mask_other_logits(self, logits: torch.Tensor, remained_ids: List[int]):
        """Mask logits to only allow specific token IDs."""
        remained_logits = {ids: logits[ids].item() for ids in remained_ids}
        new_logits = torch.ones_like(logits) * -float('inf')
        for id in remained_ids:
            new_logits[id] = remained_logits[id]
        return new_logits
        
    def calc_max_count(self, prompt_token_ids: List[int]):
        """Calculate maximum count of items from prompt."""
        pattern_list = [716, 1203, 842, 428]
        for idx in range(len(prompt_token_ids) - len(pattern_list), -1, -1):
            if all(prompt_token_ids[idx + i] == pattern_list[i] for i in range(len(pattern_list))):
                num_idx = idx + len(pattern_list)
                num_ids = []
                while num_idx < len(prompt_token_ids) and prompt_token_ids[num_idx] in self.token_id_map[TokenState.Number]:
                    num_ids.append(prompt_token_ids[num_idx])
                    num_idx += 1
                return int(self.tokenizer.decode(num_ids)) 
        return 1
        
    def find_last_complete_number(self, input_ids: List[int]):
        """Find the last complete number in input IDs."""
        if not input_ids:
            return -1, "null", -1
        
        tail_number_ids = []
        last_idx = len(input_ids) - 1
        while last_idx >= 0 and input_ids[last_idx] in self.token_id_map[TokenState.Number]:
            tail_number_ids.insert(0, input_ids[last_idx])
            last_idx -= 1
        
        tail_number = int(self.tokenizer.decode(tail_number_ids)) if tail_number_ids else -1
        
        while last_idx >= 0 and input_ids[last_idx] not in self.token_id_map[TokenState.Number]:
            last_idx -= 1
        
        if last_idx < 0:
            return tail_number, "tail", tail_number
        
        last_number_ids = []
        while last_idx >= 0 and input_ids[last_idx] in self.token_id_map[TokenState.Number]:
            last_number_ids.insert(0, input_ids[last_idx])
            last_idx -= 1
        
        last_number = int(self.tokenizer.decode(last_number_ids))
        
        if tail_number == last_number + 1:
            return tail_number, "tail", tail_number
        return last_number, "non_tail", tail_number
            
    def process_logit(self, prompt_token_ids: List[int], input_ids: List[int], logits: torch.Tensor):
        """Process logits to enforce JSON format."""
        if not input_ids:
            return self.mask_other_logits(logits, self.token_id_map[TokenState.Left_bracket])
        
        last_token = input_ids[-1]
        
        if last_token == self.token_id_map[TokenState.Right_bracket][0]:
            return self.mask_other_logits(logits, [151645])
        elif last_token == self.token_id_map[TokenState.Left_bracket][0]:
            return self.mask_other_logits(logits, self.token_id_map[TokenState.Space_quote])
        elif last_token == self.token_id_map[TokenState.Space_quote][0]:
            last_number, _, _ = self.find_last_complete_number(input_ids)
            if last_number == -1:
                next_char = '1'
            else:
                next_char = str(last_number + 1)[0]
            return self.mask_other_logits(logits, self.tokenizer.encode(next_char))
        elif last_token in self.token_id_map[TokenState.Number]:
            last_number, state, tail_number = self.find_last_complete_number(input_ids)
            if state == "tail":
                return self.mask_other_logits(logits, self.token_id_map[TokenState.Quote_colon_quote])
            else:
                next_str = str(last_number + 1)
                next_char = next_str[len(str(tail_number))]
                return self.mask_other_logits(logits, self.tokenizer.encode(next_char))
        elif last_token == self.token_id_map[TokenState.Quote_colon_quote][0]:
            return self.mask_other_logits(logits, self.token_id_map[TokenState.Main_other])
        elif last_token in self.token_id_map[TokenState.Main_other]:
            return self.mask_other_logits(logits, self.token_id_map[TokenState.Quote_comma])
        elif last_token == self.token_id_map[TokenState.Quote_comma][0]:
            last_number, _, _ = self.find_last_complete_number(input_ids)
            max_count = self.calc_max_count(prompt_token_ids)
            if last_number >= max_count:
                return self.mask_other_logits(logits, self.token_id_map[TokenState.Right_bracket])
            else:
                return self.mask_other_logits(logits, self.token_id_map[TokenState.Space_quote])
        
        return logits


@extractor("llm-webkit")
class LlmWebkitExtractor(BaseExtractor):
    """Advanced LLM-WebKit extractor with intelligent content classification."""
    
    version = "2.0.0"
    description = "Advanced LLM-WebKit extractor with intelligent content classification"
    
    # åˆ†ç±»æç¤ºæ¨¡æ¿
    CLASSIFICATION_PROMPT = """As a front-end engineering expert in HTML, your task is to analyze the given HTML structure and accurately classify elements with the _item_id attribute as either "main" (primary content) or "other" (supplementary content). Your goal is to precisely extract the primary content of the page, ensuring that only the most relevant information is labeled as "main" while excluding navigation, metadata, and other non-essential elements. 

Guidelines for Classification:

Primary Content ("main")
Elements that constitute the core content of the page should be classified as "main". These typically include:
âœ… For Articles, News, and Blogs:
The main text body of the article, blog post, or news content.
Images embedded within the main content that contribute to the article.
âœ… For Forums & Discussion Threads:
The original post in the thread.
Replies and discussions that are part of the main conversation.
âœ… For Q&A Websites:
The question itself posted by a user.
Answers to the question and replies to answers that contribute to the discussion.
âœ… For Other Content-Based Pages:
Any rich text, paragraphs, or media that serve as the primary focus of the page.

Supplementary Content ("other")
Elements that do not contribute to the primary content but serve as navigation, metadata, or supporting information should be classified as "other". These include:
âŒ Navigation & UI Elements:
Menus, sidebars, footers, breadcrumbs, and pagination links.
"Skip to content" links and accessibility-related text.
âŒ Metadata & User Information:
Article titles, author names, timestamps, and view counts.
Like counts, vote counts, and other engagement metrics.
âŒ Advertisements & Promotional Content:
Any section labeled as "Advertisement" or "Sponsored".
Social media sharing buttons, follow prompts, and external links.
âŒ Related & Suggested Content:
"Read More", "Next Article", "Trending Topics", and similar sections.
Lists of related articles, tags, and additional recommendations.

Task Instructions:
You will be provided with a simplified HTML structure containing elements with an _item_id attribute. Your job is to analyze each element's function and determine whether it should be classified as "main" or "other".

Response Format:
Return a JSON object where each key is the _item_id value, and the corresponding value is either "main" or "other", as in the following example:
{{"1": "other","2": "main","3": "other"}}

ğŸš¨ Important Notes:
Do not include any explanations in the outputâ€”only return the JSON.
Ensure high accuracy by carefully distinguishing between primary content and supplementary content.
Err on the side of cautionâ€”if an element seems uncertain, classify it as "other" unless it clearly belongs to the main content.

Input HTML:
{alg_html}

Output format should be a JSON-formatted string representing a dictionary where keys are item_id strings and values are either 'main' or 'other'. Make sure to include ALL item_ids from the input HTML."""

    def __init__(self, name: str, config: Optional[Dict[str, Any]] = None):
        super().__init__(name, config)
        self.inference_config = LLMInferenceConfig()
        self.model = None
        self.tokenizer = None
        self.token_state_manager = None
        
        # Override config if provided
        if config:
            for key, value in config.items():
                if hasattr(self.inference_config, key):
                    setattr(self.inference_config, key, value)
    
    def _setup(self) -> None:
        """Setup the LLM-WebKit extractor with advanced inference capabilities."""
        # åˆå§‹åŒ–æ¨¡å—å¼•ç”¨
        self._simplify_html = None
        self._PreDataJson = None
        self._PreDataJsonKey = None
        self._MapItemToHtmlTagsParser = None
        self._SamplingParams = None
        self._model_loaded = False
        
        # æ£€æŸ¥å„ä¸ªä¾èµ–æ¨¡å—çš„å¯ç”¨æ€§
        missing_modules = []
        
        # æ£€æŸ¥ llm_web_kit
        try:
            from llm_web_kit.main_html_parser.simplify_html.simplify_html import simplify_html
            from llm_web_kit.input.pre_data_json import PreDataJson, PreDataJsonKey
            from llm_web_kit.main_html_parser.parser.tag_mapping import MapItemToHtmlTagsParser
            
            self._simplify_html = simplify_html
            self._PreDataJson = PreDataJson
            self._PreDataJsonKey = PreDataJsonKey
            self._MapItemToHtmlTagsParser = MapItemToHtmlTagsParser
            
        except ImportError as e:
            missing_modules.append(f"llm_web_kit: {e}")
        
        # æ£€æŸ¥ transformersï¼ˆå»¶è¿Ÿåˆ°å®é™…ä½¿ç”¨æ—¶ï¼‰
        self._transformers_available = False
        try:
            import transformers
            self._transformers_available = True
        except ImportError as e:
            missing_modules.append(f"transformers: {e}")
        
        # æ£€æŸ¥ vllmï¼ˆå»¶è¿Ÿåˆ°å®é™…ä½¿ç”¨æ—¶ï¼‰
        self._vllm_available = False
        try:
            import vllm
            from vllm import SamplingParams
            self._SamplingParams = SamplingParams
            self._vllm_available = True
        except ImportError as e:
            missing_modules.append(f"vllm: {e}")
        
        # å¦‚æœå…³é”®æ¨¡å—ç¼ºå¤±ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        if missing_modules:
            error_msg = "LLM-WebKit extractor requires additional dependencies:\n"
            error_msg += "\n".join([f"  â€¢ {module}" for module in missing_modules])
            error_msg += "\n\nTo install dependencies:\n"
            error_msg += "  pip install llm_web_kit transformers vllm torch\n"
            error_msg += "\nFor CPU-only usage (limited functionality):\n"
            error_msg += "  pip install llm_web_kit transformers torch --index-url https://download.pytorch.org/whl/cpu"
            
            raise RuntimeError(error_msg)
    
    def _load_model(self):
        """å»¶è¿ŸåŠ è½½LLMæ¨¡å‹å’Œtokenizer."""
        if self._model_loaded:
            return
        
        # æ£€æŸ¥ä¾èµ–æ˜¯å¦å¯ç”¨
        if not self._transformers_available:
            raise RuntimeError("transformers library is not available. Please install it: pip install transformers")
        
        import torch
        
        # æ£€æµ‹è¿è¡Œç¯å¢ƒ
        is_apple_silicon = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
        has_cuda = torch.cuda.is_available()
        
        print(f"ğŸ” æ£€æµ‹åˆ°è¿è¡Œç¯å¢ƒ:")
        print(f"   CUDA: {has_cuda}")
        print(f"   Apple Silicon (MPS): {is_apple_silicon}")
        
        # å¯¹äºApple Siliconï¼Œä¼˜å…ˆä½¿ç”¨transformersè€Œä¸æ˜¯vLLMï¼ˆé¿å…å…¼å®¹æ€§é—®é¢˜ï¼‰
        if is_apple_silicon and not has_cuda:
            print("ğŸ Apple Siliconç¯å¢ƒæ£€æµ‹åˆ°ï¼Œä½¿ç”¨transformersæ¨¡å¼ä»¥é¿å…vLLMå…¼å®¹æ€§é—®é¢˜")
            self._load_transformers_model()
        else:
            # å…¶ä»–ç¯å¢ƒå°è¯•ä½¿ç”¨vLLM
            if not self._vllm_available:
                print("âš ï¸  vLLMä¸å¯ç”¨ï¼Œå›é€€åˆ°transformersæ¨¡å¼")
                self._load_transformers_model()
            else:
                self._load_vllm_model()
    
    def _load_transformers_model(self):
        """ä½¿ç”¨transformersåŠ è½½æ¨¡å‹ï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            print(f"ğŸ“¦ ä½¿ç”¨transformersåŠ è½½æ¨¡å‹: {self.inference_config.model_path}")
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.inference_config.model_path, 
                trust_remote_code=True
            )
            
            # è®¾ç½®è®¾å¤‡
            if torch.cuda.is_available():
                device = "cuda"
                torch_dtype = torch.bfloat16 if self.inference_config.dtype == "bfloat16" else torch.float16
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
                torch_dtype = torch.float16  # MPSç›®å‰ä¸æ”¯æŒbfloat16
            else:
                device = "cpu"
                torch_dtype = torch.float32
            
            print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}, æ•°æ®ç±»å‹: {torch_dtype}")
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.inference_config.model_path,
                trust_remote_code=True,
                torch_dtype=torch_dtype,
                device_map=device if device != "mps" else None  # MPSä¸æ”¯æŒdevice_map
            )
            
            if device == "mps":
                self.model = self.model.to(device)
            
            self.model.eval()
            
            # æ ‡è®°ä¸ºtransformersæ¨¡å¼
            self._use_transformers = True
            self._model_loaded = True
            
            print("âœ… transformersæ¨¡å‹åŠ è½½æˆåŠŸ!")
            
        except Exception as e:
            raise RuntimeError(f"Failed to load transformers model: {e}")
    
    def _load_vllm_model(self):
        """ä½¿ç”¨vLLMåŠ è½½æ¨¡å‹ï¼ˆé«˜æ€§èƒ½ä½†å…¼å®¹æ€§è¦æ±‚é«˜ï¼‰"""
        try:
            from transformers import AutoTokenizer
            from vllm import LLM
            
            print(f"âš¡ ä½¿ç”¨vLLMåŠ è½½æ¨¡å‹: {self.inference_config.model_path}")
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.inference_config.model_path, 
                trust_remote_code=True
            )
            
            # vLLMé…ç½® - å‚è€ƒray_test_qa.pyçš„ç®€åŒ–é…ç½®
            model_kwargs = {
                "model": self.inference_config.model_path,
                "trust_remote_code": True,
                "dtype": self.inference_config.dtype,
                "tensor_parallel_size": self.inference_config.tensor_parallel_size,
            }
            
            print(f"ğŸ”§ vLLMé…ç½®: {model_kwargs}")
            
            self.model = LLM(**model_kwargs)
            
            # åˆå§‹åŒ–tokençŠ¶æ€ç®¡ç†å™¨
            if self.inference_config.use_logits_processor:
                self.token_state_manager = TokenStateManager(self.tokenizer)
            
            # æ ‡è®°ä¸ºvLLMæ¨¡å¼
            self._use_transformers = False
            self._model_loaded = True
            
            print("âœ… vLLMæ¨¡å‹åŠ è½½æˆåŠŸ!")
            
        except Exception as e:
            print(f"âŒ vLLMåŠ è½½å¤±è´¥: {e}")
            raise RuntimeError(f"vLLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def _create_prompt(self, simplified_html: str) -> str:
        """åˆ›å»ºåˆ†ç±»æç¤º."""
        return self.CLASSIFICATION_PROMPT.format(alg_html=simplified_html)
    
    def _add_template(self, prompt: str) -> str:
        """æ·»åŠ èŠå¤©æ¨¡æ¿."""
        messages = [
            {"role": "user", "content": prompt}
        ]
        chat_prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=True
        )
        return chat_prompt
    
    def _generate_with_transformers(self, prompt: str) -> str:
        """ä½¿ç”¨transformersç”Ÿæˆæ–‡æœ¬"""
        try:
            import torch
            
            # Tokenizeè¾“å…¥
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=self.inference_config.max_tokens)
            
            # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            device = self.model.device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç”Ÿæˆé…ç½®
            generation_config = {
                "max_new_tokens": self.inference_config.max_output_tokens,
                "temperature": self.inference_config.temperature,
                "top_p": self.inference_config.top_p,
                "do_sample": self.inference_config.temperature > 0,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
            }
            
            print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆæ–‡æœ¬ (max_new_tokens: {generation_config['max_new_tokens']})")
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
            
            # è§£ç è¾“å‡ºï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            input_length = inputs['input_ids'].shape[1]
            generated_ids = outputs[0][input_length:]
            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(generated_text)}")
            print(f"ğŸ” LLMåŸå§‹è¾“å‡º: {repr(generated_text[:200])}")  # æ˜¾ç¤ºå‰200å­—ç¬¦ç”¨äºè°ƒè¯•
            
            # æå–JSONéƒ¨åˆ†
            json_result = self._extract_json_from_text(generated_text)
            print(f"ğŸ” æå–çš„JSON: {repr(json_result[:200])}")  # æ˜¾ç¤ºJSONç»“æœ
            return json_result
            
        except Exception as e:
            print(f"âš ï¸  transformersç”Ÿæˆå¤±è´¥: {e}")
            raise RuntimeError(f"transformersç”Ÿæˆå¤±è´¥: {e}")
    
    def _extract_json_from_text(self, text: str) -> str:
        """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–JSON"""
        # æŸ¥æ‰¾JSONéƒ¨åˆ†
        start_idx = text.find("{")
        end_idx = text.rfind("}") + 1
        
        if start_idx != -1 and end_idx != 0:
            json_str = text[start_idx:end_idx]
            # æ¸…ç†JSON
            json_str = json_str.strip()
            json_str = re.sub(r',\s*}', '}', json_str)
            try:
                # éªŒè¯JSON
                json.loads(json_str)
                return json_str
            except:
                pass
        
        return "{}"

    def _clean_output(self, output) -> str:
        """æ¸…ç†LLMè¾“å‡ºï¼Œæå–JSON."""
        prediction = output[0].outputs[0].text
        
        # æå–JSON
        start_idx = prediction.rfind("{")
        end_idx = prediction.rfind("}") + 1
        
        if start_idx != -1 and end_idx != -1:
            json_str = prediction[start_idx:end_idx]
            json_str = re.sub(r',\s*}', '}', json_str)  # æ¸…ç†JSON
            try:
                json.loads(json_str)  # éªŒè¯
                return json_str
            except:
                return "{}"
        else:
            return "{}"
    
    def _reformat_classification_result(self, json_str: str) -> Dict[str, int]:
        """é‡æ–°æ ¼å¼åŒ–åˆ†ç±»ç»“æœ."""
        try:
            data = json.loads(json_str)
            return {"item_id " + k: 1 if v == "main" else 0 for k, v in data.items()}
        except json.JSONDecodeError:
            return {}
    
    def _reconstruct_content(self, original_html: str, classification_result: Dict[str, int], url: str = None) -> tuple:
        """æ ¹æ®åˆ†ç±»ç»“æœé‡å»ºä¸»è¦å†…å®¹."""
        try:
            # æŒ‰ç…§ray_test_qa.pyçš„æ­£ç¡®æµç¨‹
            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨MapItemToHtmlTagsParserç”Ÿæˆmain_html
            main_html = self._generate_main_html_with_parser(original_html, classification_result)
            print(f"ğŸ”§ MapItemToHtmlTagsParserç”Ÿæˆçš„main_htmlé•¿åº¦: {len(main_html)}")
            
            if not main_html.strip():
                print("âš ï¸  æ²¡æœ‰ç”Ÿæˆmain_htmlï¼Œè¿”å›ç©ºç»“æœ")
                return "", []
            
            # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨llm-webkitçš„æ–¹æ³•å°†main_htmlæå–æˆcontentï¼Œä¼ å…¥URL
            content, content_list = self._extract_content_from_main_html(main_html, url)
            print(f"âœ… contentæå–æˆåŠŸ: {len(content)}å­—ç¬¦, {len(content_list)}ä¸ªå†…å®¹å—")
            
            return content, content_list
            
        except Exception as e:
            print(f"âŒ Content reconstruction failed: {e}")
            return "", []
    
    def _generate_main_html_with_parser(self, original_html: str, classification_result: Dict[str, int]) -> str:
        """ä½¿ç”¨MapItemToHtmlTagsParserç”Ÿæˆmain_htmlï¼ˆæŒ‰ç…§ray_test_qa.pyçš„æµç¨‹ï¼‰"""
        try:
            # è·å–typical_raw_tag_html (ç®€åŒ–çš„HTML)
            simplified_html, typical_raw_tag_html, _ = self._simplify_html(original_html)
            print(f"ğŸ”§ simplified HTMLé•¿åº¦: {len(simplified_html)}")
            print(f"ğŸ”§ typical_raw_tag_htmlé•¿åº¦: {len(typical_raw_tag_html)}")
            
            # æŒ‰ç…§ray_test_qa.pyçš„æµç¨‹
            pre_data = self._PreDataJson({})
            pre_data[self._PreDataJsonKey.LLM_RESPONSE] = classification_result
            pre_data[self._PreDataJsonKey.TYPICAL_RAW_HTML] = original_html
            pre_data[self._PreDataJsonKey.TYPICAL_RAW_TAG_HTML] = typical_raw_tag_html
            
            print(f"ğŸ”§ PreDataJsonè®¾ç½®å®Œæˆï¼Œå¼€å§‹è§£æ...")
            
            # ä½¿ç”¨MapItemToHtmlTagsParserè§£æ
            parser = self._MapItemToHtmlTagsParser({})
            pre_data = parser.parse_single(pre_data)
            
            # è·å–ç”Ÿæˆçš„main_html
            main_html = pre_data.get(self._PreDataJsonKey.TYPICAL_MAIN_HTML, "")
            
            print(f"âœ… MapItemToHtmlTagsParserå®Œæˆï¼Œmain_htmlé•¿åº¦: {len(main_html)}")
            return main_html
            
        except Exception as e:
            print(f"âŒ MapItemToHtmlTagsParserå¤±è´¥: {e}")
            return ""
    
    def _extract_content_from_main_html(self, main_html: str, url: str = None) -> tuple:
        """ä½¿ç”¨llm-webkitçš„æ–¹æ³•å°†main_htmlæå–æˆcontent"""
        try:
            from llm_web_kit.simple import extract_html_to_md
            import traceback
            
            print(f"ğŸ”§ å¼€å§‹ä½¿ç”¨llm-webkitç®€å•æ¥å£æå–content...")
            
            # ä½¿ç”¨ç®€å•æ¥å£æå–markdownï¼Œä¼ å…¥URL
            content = extract_html_to_md(url or "", main_html, clip_html=False)
            
            print(f"âœ… llm-webkitæå–å®Œæˆ: {len(content)}å­—ç¬¦")
            
            # æš‚ä¸æ„å»ºcontent_listï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
            return content.strip(), []
            
        except Exception as e:
            print(f"âŒ llm-webkitæå–å¤±è´¥: {e}")
            print(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return "", []
    
    
    def _extract_content(self, html: str, url: str = None) -> ExtractionResult:
        """
        ä½¿ç”¨é«˜çº§LLMæ¨ç†æå–å†…å®¹.
        
        Args:
            html: HTMLå†…å®¹
            url: å¯é€‰çš„é¡µé¢URL
            
        Returns:
            ExtractionResultå®ä¾‹
        """
        start_time = time.time()
        
        try:
            # æ­¥éª¤1: HTMLç®€åŒ–å¤„ç†
            simplified_html, typical_raw_tag_html, _ = self._simplify_html(html)
            
            # æ­¥éª¤2: æ£€æŸ¥é•¿åº¦é™åˆ¶
            item_count = simplified_html.count('_item_id')
            if item_count > self.inference_config.max_item_count:
                return ExtractionResult.create_error_result(
                    f"HTML too complex: {item_count} items > {self.inference_config.max_item_count} limit"
                )
            
            if item_count == 0:
                return ExtractionResult.create_error_result("No _item_id found in simplified HTML")
            
            # æ­¥éª¤3: å»¶è¿ŸåŠ è½½æ¨¡å‹
            self._load_model()
            
            # æ­¥éª¤4: åˆ›å»ºæç¤ºå¹¶è¿›è¡ŒLLMæ¨ç†
            prompt = self._create_prompt(simplified_html)
            chat_prompt = self._add_template(prompt)
            
            # é…ç½®é‡‡æ ·å‚æ•°
            if self.inference_config.use_logits_processor and self.token_state_manager:
                sampling_params = self._SamplingParams(
                    temperature=self.inference_config.temperature,
                    top_p=self.inference_config.top_p,
                    max_tokens=self.inference_config.max_output_tokens,
                    logits_processors=[self.token_state_manager.process_logit]
                )
            else:
                sampling_params = self._SamplingParams(
                    temperature=self.inference_config.temperature,
                    top_p=self.inference_config.top_p,
                    max_tokens=self.inference_config.max_output_tokens
                )
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç”Ÿæˆæ–¹å¼
            if hasattr(self, '_use_transformers') and self._use_transformers:
                # ä½¿ç”¨transformersç”Ÿæˆ
                json_result = self._generate_with_transformers(chat_prompt)
            else:
                # ä½¿ç”¨vLLMç”Ÿæˆ
                output = self.model.generate(chat_prompt, sampling_params)
                json_result = self._clean_output(output)
            
            # æ­¥éª¤5: æ ¼å¼è½¬æ¢å’Œå†…å®¹é‡å»º
            print(f"ğŸ”„ å¼€å§‹æ ¼å¼è½¬æ¢...")
            classification_result = self._reformat_classification_result(json_result)
            print(f"ğŸ” æ ¼å¼è½¬æ¢ç»“æœ: {len(classification_result)} ä¸ªåˆ†ç±»é¡¹")
            
            print(f"ğŸ”„ å¼€å§‹é‡å»ºå†…å®¹...")
            main_content, content_list = self._reconstruct_content(html, classification_result, url)
            print(f"ğŸ” é‡å»ºç»“æœ: ä¸»å†…å®¹é•¿åº¦={len(main_content)}, å†…å®¹å—æ•°é‡={len(content_list) if content_list else 0}")
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(main_content, content_list, item_count)
            
            extraction_time = time.time() - start_time
            
            # åˆ›å»ºç»“æœå¯¹è±¡
            result = ExtractionResult(
                content=main_content,
                # content_list=content_list,
                title=self._extract_title(html),
                language=self._detect_language(main_content),
                confidence_score=confidence,
                extraction_time=extraction_time,
                success=True
            )
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯åˆ°é”™è¯¯æ¶ˆæ¯å­—æ®µï¼ˆç”¨äºå¼€å‘è°ƒè¯•ï¼‰
            debug_info = f"item_count: {item_count}, llm_output_length: {len(json_result)}"
            if not result.success:
                result.error_message = f"{result.error_message or ''} | {debug_info}".strip(' |')
            
            return result
            
        except Exception as e:
            extraction_time = time.time() - start_time
            return ExtractionResult.create_error_result(
                f"LLM-WebKit extraction failed: {str(e)}",
                extraction_time=extraction_time
            )
    
    def _extract_title(self, html: str) -> Optional[str]:
        """æå–é¡µé¢æ ‡é¢˜."""
        try:
            import re
            title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
            if title_match:
                return title_match.group(1).strip()
        except:
            pass
        return None
    
    def _detect_language(self, content: str) -> Optional[str]:
        """æ£€æµ‹å†…å®¹è¯­è¨€."""
        if not content:
            return None
            
        # ç®€å•çš„è¯­è¨€æ£€æµ‹é€»è¾‘
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
        english_chars = len(re.findall(r'[a-zA-Z]', content))
        
        if chinese_chars > english_chars:
            return "zh"
        elif english_chars > 0:
            return "en"
        else:
            return None
    
    def _calculate_confidence(self, content: str, content_list: List[Dict], item_count: int) -> float:
        """è®¡ç®—æå–ç½®ä¿¡åº¦."""
        if not content:
            return 0.0
        
        # åŸºäºå†…å®¹é•¿åº¦çš„è¯„åˆ†
        length_score = min(len(content) / 1000, 1.0)
        
        # åŸºäºç»“æ„åŒ–å†…å®¹çš„è¯„åˆ†
        structure_score = min(len(content_list) / 10, 1.0) if content_list else 0.0
        
        # åŸºäºå¤„ç†å¤æ‚åº¦çš„è¯„åˆ†ï¼ˆitemæ•°é‡è¶Šå¤šï¼Œç½®ä¿¡åº¦ç¨å¾®é™ä½ï¼‰
        complexity_penalty = max(0, (item_count - 100) / 900)  # 100-1000èŒƒå›´å†…çº¿æ€§é™ä½
        complexity_score = max(0.5, 1.0 - complexity_penalty)
        
        # ç»¼åˆè¯„åˆ†
        confidence = (length_score * 0.5 + structure_score * 0.3 + complexity_score * 0.2)
        return min(confidence, 1.0) 